import gradio as gr
import os
import time
import torch
import numpy as np
from diffusers import DiffusionPipeline, AutoPipelineForText2Image
from diffusers.utils import export_to_video
import warnings
warnings.filterwarnings("ignore")


def check_gpu_availability():
    """Verificar disponibilidade da GPU"""
    print("ğŸ”§ Verificando GPU...")

    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3

        print(f"âœ… CUDA disponÃ­vel!")
        print(f"ğŸ® GPU: {gpu_name}")
        print(f"ğŸ’¾ VRAM Total: {gpu_memory:.1f}GB")
        print(f"ğŸ”¢ GPUs disponÃ­veis: {gpu_count}")

        # Testar alocaÃ§Ã£o GPU
        try:
            test_tensor = torch.randn(100, 100).cuda()
            print(f"ğŸ§ª Teste de alocaÃ§Ã£o GPU: âœ…")
            del test_tensor
            torch.cuda.empty_cache()
            return True
        except Exception as e:
            print(f"âŒ Falha no teste de GPU: {e}")
            return False
    else:
        print("âŒ CUDA nÃ£o disponÃ­vel - usando CPU")
        return False


def load_wan_model():
    """Carregar modelo WAN usando diffusers (forma correta)"""

    # Verificar GPU primeiro
    gpu_available = check_gpu_availability()
    device = "cuda" if gpu_available else "cpu"

    print(f"\nğŸ“¥ Carregando WAN Model via diffusers...")
    print(f"ğŸ“‚ RepositÃ³rio: Comfy-Org/Wan_2.1_ComfyUI_repackaged")
    print(f"ğŸ¯ Dispositivo alvo: {device}")
    print(f"ğŸ’¡ Usando diffusers (mÃ©todo correto para WAN)")

    try:
        # Tentar carregar como pipeline de diffusers
        print("ğŸ”§ Tentando carregar como DiffusionPipeline...")

        pipeline = DiffusionPipeline.from_pretrained(
            "Comfy-Org/Wan_2.1_ComfyUI_repackaged",
            cache_dir="/app/models",
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            trust_remote_code=True
        )

        if device == "cuda":
            pipeline = pipeline.to("cuda")
            print("âœ… Pipeline movido para GPU")

        print(f"âœ… WAN Model carregado via diffusers no {device.upper()}!")
        return pipeline, f"WAN 2.1 (Diffusers - {device.upper()})", device, "diffusers"

    except Exception as e:
        print(f"âŒ Erro ao carregar via diffusers: {e}")

        # Tentar como AutoPipeline
        try:
            print("ğŸ”„ Tentando AutoPipelineForText2Image...")

            pipeline = AutoPipelineForText2Image.from_pretrained(
                "Comfy-Org/Wan_2.1_ComfyUI_repackaged",
                cache_dir="/app/models",
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                trust_remote_code=True
            )

            if device == "cuda":
                pipeline = pipeline.to("cuda")

            print(
                f"âœ… WAN Model carregado via AutoPipeline no {device.upper()}!")
            return pipeline, f"WAN 2.1 (AutoPipeline - {device.upper()})", device, "auto"

        except Exception as e2:
            print(f"âŒ Erro no AutoPipeline: {e2}")

            # Verificar se Ã© realmente um modelo de texto
            try:
                print("ğŸ”„ Tentando carregar componentes individuais...")
                from transformers import AutoTokenizer, AutoModelForCausalLM

                # Tentar carregar como modelo de texto tradicional
                tokenizer = AutoTokenizer.from_pretrained(
                    "Comfy-Org/Wan_2.1_ComfyUI_repackaged",
                    cache_dir="/app/models",
                    trust_remote_code=True
                )

                model = AutoModelForCausalLM.from_pretrained(
                    "Comfy-Org/Wan_2.1_ComfyUI_repackaged",
                    cache_dir="/app/models",
                    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                    device_map="auto" if device == "cuda" else None,
                    trust_remote_code=True
                )

                from transformers import pipeline
                text_pipeline = pipeline(
                    "text-generation",
                    model=model,
                    tokenizer=tokenizer,
                    torch_dtype=torch.float16 if device == "cuda" else torch.float32
                )

                print(
                    f"âœ… WAN carregado como modelo de texto no {device.upper()}!")
                return text_pipeline, f"WAN 2.1 (Text - {device.upper()})", device, "text"

            except Exception as e3:
                print(f"âŒ Falha final: {e3}")
                return None, "Erro no carregamento", "cpu", "none"


print("ğŸ¤– Inicializando WAN Model Otimizado...")
print("ğŸ¯ Objetivo: Usar modelo WAN especÃ­fico na RTX 3060")
print("ğŸ”§ MÃ©todo: diffusers (forma correta para WAN)")

model_description = "NÃ£o carregado"
device_info = "cpu"
model_type = "none"
pipeline = None

try:
    pipeline, model_description, device_info, model_type = load_wan_model()

except Exception as e:
    print(f"âŒ Erro geral no carregamento: {e}")
    pipeline = None


def generate_content(prompt, max_tokens=512, temperature=0.7, top_p=0.9):
    if not pipeline:
        return f"""âŒ Modelo WAN nÃ£o carregado.

ğŸ” **DiagnÃ³stico:**
- Falha ao carregar WAN via diffusers
- Modelo pode nÃ£o estar no formato esperado
- Verificar se repositÃ³rio Ã© realmente um modelo WAN funcional

ğŸ’¡ **SoluÃ§Ãµes:**
1. Verificar se repositÃ³rio tem modelo WAN vÃ¡lido
2. Usar ComfyUI para modelos WAN T2V especÃ­ficos
3. Tentar repositÃ³rio alternativo do WAN

ğŸ“Š **Status:** {model_description}
ğŸ”§ **Dispositivo:** {device_info}
ğŸ­ **Tipo:** {model_type}
"""

    try:
        start_time = time.time()

        # Verificar se estÃ¡ realmente usando GPU
        if device_info == "cuda":
            print(f"ğŸ® Gerando com WAN na GPU...")
            torch.cuda.empty_cache()
        else:
            print(f"ğŸ’» Gerando com WAN na CPU...")

        # GeraÃ§Ã£o baseada no tipo de modelo
        if model_type == "diffusers" or model_type == "auto":
            # Para modelos de difusÃ£o (geraÃ§Ã£o de imagem/vÃ­deo)
            result = pipeline(
                prompt=prompt,
                num_inference_steps=20,
                guidance_scale=7.5,
                height=512,
                width=512
            )

            if hasattr(result, 'images'):
                response = f"âœ… Imagem gerada com WAN Model!\nğŸ–¼ï¸ ResoluÃ§Ã£o: 512x512\nğŸ“ Prompt: {prompt}"
            else:
                response = f"âœ… ConteÃºdo gerado com WAN Model!\nğŸ“ Prompt: {prompt}"

        elif model_type == "text":
            # Para modelos de texto
            generation_config = {
                "max_new_tokens": min(max_tokens, 512),
                "temperature": temperature,
                "top_p": top_p,
                "do_sample": True,
                "return_full_text": False
            }

            result = pipeline(prompt, **generation_config)

            if isinstance(result, list) and len(result) > 0:
                response = result[0]["generated_text"].strip()
            else:
                response = "Erro na extraÃ§Ã£o do texto gerado"
        else:
            response = "Tipo de modelo nÃ£o suportado"

        end_time = time.time()

        # Calcular estatÃ­sticas
        time_taken = end_time - start_time

        # Adicionar info da GPU
        gpu_status = ""
        if device_info == "cuda":
            try:
                gpu_memory_used = torch.cuda.memory_allocated(0) / 1024**3
                gpu_memory_cached = torch.cuda.memory_reserved(0) / 1024**3
                gpu_status = f"\nğŸ® GPU Usado: {gpu_memory_used:.1f}GB | Cache: {gpu_memory_cached:.1f}GB"
            except:
                gpu_status = "\nğŸ® GPU: Ativo"

        stats = f"\n\n---\nğŸ“Š **EstatÃ­sticas WAN:**\nâ±ï¸ Tempo: {time_taken:.2f}s\nğŸ¤– Modelo: {model_description}\nğŸ”§ Dispositivo: {device_info.upper()}\nğŸ­ Tipo: {model_type.upper()}{gpu_status}"

        # Limpar cache GPU apÃ³s geraÃ§Ã£o
        if device_info == "cuda":
            torch.cuda.empty_cache()

        return response + stats

    except Exception as e:
        return f"âŒ Erro na geraÃ§Ã£o WAN: {str(e)}\n\nğŸ¤– Modelo: {model_description}\nğŸ”§ Dispositivo: {device_info}"


# Interface Gradio
with gr.Blocks(
    title="WAN Model Otimizado - RTX 3060",
    theme=gr.themes.Soft(),
    css="""
    .gradio-container {
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    }
    """
) as demo:
    gr.Markdown(f"""
    # ğŸš€ WAN Model Otimizado - RTX 3060 12GB
    
    **Modelo WAN executando via diffusers/transformers**
    
    ğŸ“Š **EspecificaÃ§Ãµes:**
    - GPU: NVIDIA RTX 3060 (12GB VRAM)
    - Modelo: {model_description}
    - Backend: diffusers/transformers
    - Dispositivo: {device_info}
    - Tipo: {model_type}
    
    âœ… **RepositÃ³rio:** [Comfy-Org/Wan_2.1_ComfyUI_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged)
    """)

    with gr.Row():
        with gr.Column(scale=1):
            prompt_input = gr.Textbox(
                label="ğŸ’¬ Prompt para WAN Model",
                placeholder="Digite seu prompt para o modelo WAN...",
                lines=6,
                value="A futuristic city with flying cars at sunset"
            )

            with gr.Row():
                max_tokens = gr.Slider(
                    minimum=50,
                    maximum=1024,
                    value=256,
                    step=50,
                    label="ğŸ“ Max Tokens (para texto)"
                )
                temperature = gr.Slider(
                    minimum=0.1,
                    maximum=2.0,
                    value=0.8,
                    step=0.1,
                    label="ğŸŒ¡ï¸ Temperature"
                )

            top_p = gr.Slider(
                minimum=0.1,
                maximum=1.0,
                value=0.9,
                step=0.05,
                label="ğŸ¯ Top P"
            )

            with gr.Row():
                generate_btn = gr.Button(
                    "ğŸš€ Gerar com WAN", variant="primary", size="lg")
                clear_btn = gr.Button("ğŸ—‘ï¸ Limpar", variant="secondary")

        with gr.Column(scale=1):
            output_text = gr.Textbox(
                label="ğŸ¤– Resultado do WAN Model",
                lines=20,
                interactive=False,
                show_copy_button=True
            )

    # Exemplos especÃ­ficos para WAN
    gr.Markdown("### ğŸ’¡ Exemplos de Prompts para WAN:")
    examples = [
        "A robot walking through a neon-lit cyberpunk street",
        "Ocean waves crashing against cliffs during a storm",
        "A magical forest with glowing mushrooms and fireflies",
        "Futuristic spacecraft landing on an alien planet",
        "Abstract geometric patterns morphing and flowing"
    ]

    with gr.Row():
        for i, example in enumerate(examples[:3]):
            btn = gr.Button(f"ğŸ“ {example[:25]}...", size="sm")
            btn.click(lambda x=example: x, outputs=prompt_input)

    # Event handlers
    generate_btn.click(
        fn=generate_content,
        inputs=[prompt_input, max_tokens, temperature, top_p],
        outputs=output_text
    )

    clear_btn.click(
        lambda: ("", ""),
        outputs=[prompt_input, output_text]
    )

if __name__ == "__main__":
    print("ğŸŒ Iniciando interface WAN...")
    port = int(os.environ.get('GRADIO_SERVER_PORT', 7861))
    print(f"ğŸ”— Acesse: http://localhost:{port}")
    demo.launch(
        server_name="0.0.0.0",
        server_port=port,
        share=False,
        show_api=False,
        inbrowser=False
    )
